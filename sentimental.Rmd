---
title: "sentimental_analysis"
output:
  pdf_document: default
  word_document: default
  html_document: default
date: "2023-04-03"
---

```{r}
library(gutenbergr)
library(tidytext)
library(janeaustenr)
library(tidyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(scales)
library(SnowballC)
library(here)
library(textdata)
```
triebeard, urltools
```{r}
gutenberg_works(title== "The Complete Works of William Shakespeare")
```
```{r}
#saveRDS(william, file = "william.rds")
william <- readRDS("william.rds")
```
```{r}
cleaned_william <- william[58:nrow(william), ] |> mutate(reg_ex =  
  sub("[A-Z ]*\\.", "", text))
cleaned_william <- cleaned_william |> mutate(reg_ex = sub("(\\[.*?\\])", "", reg_ex))
cleaned_william$reg_ex[grep("Enter", cleaned_william$reg_ex, fixed = TRUE)] <- ""
```

trimws() trim the whitespace between them
line 33 rename the value of the chapter 10 to be matched with the actual text
```{r}
chapter <- william[10:53,2]
chapter <- pull(chapter, text) |> trimws()
#chapter[10] <- "THE SECOND PART OF KING HENRY IV"
```

```{r}
cleaned_chapter <- lapply(chapter, grep, cleaned_william$text, fixed = TRUE)
```



```{r}
first_line <- integer()
for (i in 1:length(cleaned_chapter)){
     first_line[[i]] <- cleaned_chapter[[i]][1]
}
```
```{r}
last_line <- Hmisc::Lag(first_line, shift =-1)
last_line <- last_line -1
last_line[44] <- nrow(cleaned_william)
```


```{r}
chapter_n <- data.frame(first_line, last_line, chapter)
```
```{r}
cleaned_william$play <- NA
for (i in 1:nrow(chapter_n)){
  cleaned_william[chapter_n$first_line[i]:chapter_n$last_line[i], 4] <- chapter_n$chapter[i]
}
# Subset Rows by column value
subset_df <- function(name) {
  play <- cleaned_william[cleaned_william$play == name,]
}

```
```{r}
#loop for creating the play column in the text in order to tokenize them
cleaned_william <- cleaned_william |> group_by(play) |> mutate(linenumber = row_number()) |> ungroup()
```
```{r}
#load stop_words and add more into stop_words such as common noun and name
data("stop_words")
new_words <- data.frame(word = c("king", "hamlet", "antony", "richard", "othello", "romeo", "caesar", "macbeth", "cleopatra", "edward", "sir", "timon", "lear", "lord", "coriolanus", "juliet", "prince", "thou", "thy", "thee"))
stop_words <- stop_words |> select(-lexicon)
stop_words <- rbind(stop_words, new_words)

```
```{r}
tokenized <- cleaned_william |> unnest_tokens(word, reg_ex) |> anti_join(stop_words) |> mutate(word = str_extract(word, "[a-z']+")) |> mutate(stem = wordStem(word)) |> count(play, stem, sort = TRUE) 

```



```{r}
#Get total words in each play
total_words <- tokenized |>
  group_by(play) |> dplyr::summarize(total = sum(n))

countbyplay <- tokenized
combined_freq <- countbyplay |> left_join(total_words, by = "play")
```
```{r}
#get ifr
getifr <- combined_freq |> mutate(relativefreq = n/total, ifr = log(relativefreq))

tf_idf <- combined_freq |> bind_tf_idf(stem, play, n)
```

```{r}
#Graph
library(forcats)

p_out <- tf_idf |> 
  filter(play %in% chapter_n$chapter[1:6]) |> 
  group_by(play) |> 
  slice_max(tf_idf, n =15) |>
  ungroup() |>
  ggplot(aes(tf_idf, fct_reorder(stem, tf_idf), fill = play)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~play, ncol = 2, scales = "free") +
  labs(x = "tf_idf", y = "NULL") 
ggsave("tf.png", plot = p_out, width = 10, height= 15)
```
```{r}
tf_idf_w <- tf_idf |>
  group_by(play) |>
  slice_max(tf_idf, n = 50) |>
  ungroup() |>
  pivot_wider(names_from = play, values_from = tf_idf)
```

```{r}

#Analysis only for Tragedy play
#subset tragedy play

subset_words <- c("TRAGEDY")
tragedy <- subset(chapter, grepl(paste(subset_words, collapse = "|"), chapter))
```



```{r}
#Graph
library(forcats)

p_out <- tf_idf |> 
  filter(play %in% tragedy) |> 
  group_by(play) |> 
  slice_max(tf_idf, n =15) |>
  ungroup() |>
  ggplot(aes(tf_idf, fct_reorder(stem, tf_idf), fill = play)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~play, ncol = , scales = "free") +
  labs(x = "tf_idf", y = "NULL") 
ggsave("tf.png", plot = p_out, width = 10, height= 15)
```
```{r}
tf_idf_w <- tf_idf |>
  group_by(play) |>
  slice_max(tf_idf, n = 50) |>
  ungroup() |>
  pivot_wider(names_from = play, values_from = tf_idf)
```
```{r}
library(topicmodels)
tragedy_play <- tf_idf |> 
  filter(play %in% tragedy) |> 
  cast_dtm(play, stem, n)
tragedy_lda <- LDA(tragedy_play, k = 2, control = list(seed = 1234))
```
```{r}
tragedy_topics <- tidy(tragedy_lda, matrix = "beta")
```
```{r}
tragedy_top_term <- tragedy_topics |>
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() |>
  arrange(topic, -beta)
```
```{r}
tragedy_top_term |> mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
```{r}
beta_wide <- tragedy_topics |> 
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |>
  filter(topic1 > .001 | topic2 > .001) |>
  mutate(log_ratio =log2(topic2 / topic1))
```

```{r}
beta_wide |> arrange(-1*log_ratio) |>
  slice_head(n = 30) |>
  ggplot(aes(x = reorder(term, log_ratio), y= log_ratio)) +
  geom_col(show.legend = FALSE, orientation = "x") +
  coord_flip() +
  ggtitle(("Highest log ratio"))

```
```{r}
beta_wide |> arrange(log_ratio) |>
  slice_head(n = 30) |>
  ggplot(aes(x = reorder(term, log_ratio), y = log_ratio)) +
  geom_col(show.legend = FALSE, orientation = "x") +
  coord_flip() + 
  ggtitle("Lowest log ratio")
```
```{r}
tragedy_documents <- tidy(tragedy_lda, matrix = "gamma")
```
```{r}
tragedy_lda3 <- LDA(tragedy_play, k =3, control= list(seed = 1234))
tragedy_documents <- tidy(tragedy_lda3, matrix = "beta")

tragedy_top_terms <- tragedy_documents |>
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() |>
  arrange(topic, -beta)

tragedy_top_terms |> 
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_y_reordered()
```
```{r}
tragedy_documents_gamma <- tidy(tragedy_lda3, matrix = "gamma")
tragedy_documents_gamma

```
```{r}
beta_wide3 <- tragedy_documents |>
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |>
  filter(topic1 >.001 | topic2 > .001) |>
  mutate(log_ratio_21 = log2(topic2/topic1),
         log_ratio_23 = log2(topic2/topic3),
         log_ratio_13 = log2(topic1/topic3))
```
```{r}
beta_wide3 |> arrange(-log_ratio_21) |>
  slice_head(n =30) |>
  ggplot(aes(x = reorder(term, log_ratio_21), y = log_ratio_21)) +
  geom_col(show.legend = FALSE, orientation = "x") +
  coord_flip() +
  ggtitle("Highest log ratio")

beta_wide3 |> arrange(log_ratio_21) |>
  slice_head(n =30) |>
  ggplot(aes(x = reorder(term, log_ratio_21), y = log_ratio_21)) +
  geom_col(show.legend = FALSE, orientation = "x") +
  coord_flip() +
  ggtitle("Lowest log ratio")
```
```{r}
tragedy_lda5 <- LDA(tragedy_play, k = 5, control= list(seed = 1234))
tragedy_documents <- tidy(tragedy_lda5, matrix = "beta")

tragedy_top_terms <- tragedy_documents |>
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() |>
  arrange(topic, -beta)

tragedy_top_terms |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_y_reordered()
```
```{r}
tragedy_documents_gamma <- tidy(tragedy_lda, matrix = "gamma")
tragedy_documents_gamma
```
```{r}
play_topics <- tragedy_documents_gamma |>
  count(document, topic) |>
  group_by(document) |>
  slice_max(n, n = 1)|>
  ungroup() |>
  mutate(consensus = document, topic)
```
```{r}
tragedy_documents_gamma |>
  inner_join(play_topics, by = "topic") 
```


#Word Assignments
```{r}
play_dtm <- tokenized |> filter(play %in% tragedy) |> cast_dtm(play, stem, n)
play_lda <- LDA(play_dtm, k = 7, control = list(seed = 1234))
play_topics <- tidy(play_lda, matrix = "beta")

top_terms <- play_topics |> group_by(topic) |>
  slice_max(beta, n =10) |> 
  ungroup() |>
  arrange(topic, -beta)

top_terms |> 
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

play_gamma <- tidy(play_lda, matrix = "gamma")

play_gamma |> mutate(play = reorder(document, gamma*topic)) |>
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() + 
  facet_wrap(~ play) +
  labs(x = "topic", y = expression(gamma))

```
```{r}
play_classification <- play_gamma |>
  group_by(document) |>
  slice_max(gamma) |>
  ungroup()

play_topic <- play_classification |> 
  count(document, topic) |>
  group_by(document) |>
  slice_max(n, n = 1) |>
  ungroup() |>
  transmute(consensus = document, topic)

play_classification |> inner_join(play_topic, by = "topic") |>
  filter(document != consensus)

assignments <- augment(play_lda, data= play_dtm)
assignments <- assignments |> inner_join(play_topic, by = c(".topic" = "topic"))

library(scales)

assignments |>
  count(document, consensus, wt = count) |>
  mutate(across(c(document, consensus), ~str_wrap(., 20))) |>
  group_by(document) |>
  mutate(percent = n/sum(n)) |>
  ggplot(aes(consensus, document, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "darkred", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Play words were assigned to",
       y = "Play words came from",
       fill = "% off assignments")
```
```{r}
#Sentimental Analysis
get_sentiments(lexicon = c("bing", "afinn", "loughran", "nrc"))


#Every play's sentiments

tokenized_without_stem <- cleaned_william |>
  filter(play %in% tragedy) |>
  group_by(play) |>
  mutate(linenumber = row_number()) |>
  ungroup() |>
  unnest_tokens(word, reg_ex) |> 
  mutate(word = str_extract(word, "[a-z']+"))

tokenized_without_stem |> 
  count(play, word, sort = TRUE)

#Words in plays
bing_word_counts <- tokenized_without_stem |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  ungroup()

william_sentiment <- tokenized_without_stem |>
  inner_join(get_sentiments("bing")) |>
  count(play, index = linenumber%/% 20, sentiment) |>
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  mutate(sentiment = positive - negative)

p_out <- ggplot(william_sentiment, aes(index, sentiment, fill = play)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~play, ncol = 3, scales = "free_x")
ggsave("sentiments.png", plot = p_out, width = 15, height = 10)
```
```{r}
library(wordcloud)
library(reshape2)
tokenized_without_stem |> 
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  acast(word ~ sentiment, value.var = "n", fill = 0) |>
  comparison.cloud(colors = c("#E35C18", "#4091BF"),
                   max.words = 150)
```

